
import sys

sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")


import ast
from typing import List
import re
import jieba.analyse
import openai
import pandas as pd
from langchain import LLMChain, OpenAI, PromptTemplate, text_splitter
from langchain.agents import AgentOutputParser, LLMSingleActionAgent, AgentExecutor, initialize_agent, AgentType
from langchain.callbacks import StreamlitCallbackHandler
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import StringPromptTemplate
from langchain.schema import AgentFinish, AgentAction
from langchain.text_splitter import CharacterTextSplitter
from langchain.tools import Tool
import streamlit as st
from langchain.vectorstores import Chroma
import PyPDF2
import tempfile
import os
import split
import requests
import json
api_key='8j9mqPr37oHsORDKJTWyeYMdBGgA5cZz'

def on_file_change(file):
    # åœ¨è¿™é‡Œå¤„ç†æ–‡ä»¶ä¸Šä¼ åçš„æ“ä½œ
    return f'æ–‡ä»¶å: {file.name},æ–‡ä»¶å¤§å°: {file.size} bytes'
styl = """
<style>

    .stButton{
        position: fixed;
        bottom: 1rem;
        left:500;
        right:500;
        z-index:999;
    }

    @media screen and (max-width: 1000px) {

        .stButton {
            left:2%;
            width: 100%;
            bottom:1.1rem;
            z-index:999;
        }
    }

</style>

"""

st.markdown(styl, unsafe_allow_html=True)

st.title("ğŸ’¬ å¤–æ–‡æ–‡çŒ®åŠ©æ‰‹")
st.caption('ä½ å¯ä»¥è”ç½‘æŸ¥è¯¢å’Œä¸‹è½½ç›¸å…³é¢†åŸŸçš„å¤–æ–‡æ–‡çŒ®ï¼Œå¹¶æ’°å†™è®ºæ–‡ç»¼è¿°ã€‚ä½ ä¹Ÿå¯ä»¥ä¸Šä¼ ä¸€ä¸ªpdf,å¯¹å…¶è¿›è¡Œåˆ†æ')
uploaded_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ªçº¯æ–‡æœ¬docxæ–‡ä»¶æˆ–è€…pdfæ–‡ä»¶",accept_multiple_files=False,label_visibility="hidden")
if uploaded_file is None:
    st.cache_resource.clear()
if "messages_article" not in st.session_state:
    st.session_state["messages_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
if "å›ç­”å†…å®¹_article" not in st.session_state:
    st.session_state["å›ç­”å†…å®¹_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
if 'å›ç­”æ¬¡æ•°_article' not in st.session_state:
    st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
if "messages_wikipedia_strings" not in st.session_state:
    st.session_state["messages_wikipedia_strings"] = []
# if "messages_wikipedia_strings_read" not in st.session_state:
#     st.session_state["messages_wikipedia_strings_read"] = []
# if "messages_prompt" not in st.session_state:
#     st.session_state["messages_prompt"] = []
if "messages_wikipedia_ç¼“å­˜å…³é”®è¯" not in st.session_state:
    st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"] = []

if st.button('é‡æ–°å¼€å§‹ä¸€ä¸ªå›ç­”'):
    del st.session_state["å›ç­”å†…å®¹_article"]
    del st.session_state["messages_article"]
    del st.session_state["å›ç­”æ¬¡æ•°_article"]
    del st.session_state["messages_wikipedia_strings"]
    # del st.session_state["messages_wikipedia_strings_read"]
    # del st.session_state["messages_prompt"]
    del st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"]
    st.session_state["å›ç­”å†…å®¹_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
    st.session_state["messages_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
    st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
    st.session_state["messages_wikipedia_strings"] = []
    # st.session_state["messages_wikipedia_strings_read"] = []
    # st.session_state["messages_prompt"] = []
    st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"]=[]
    # æ¸…ç©ºæ–‡æœ¬è¾“å…¥æ¡†çš„å†…å®¹
    user_input = ""

for msg in st.session_state["messages_article"]:
    st.chat_message(msg["role"]).write(msg["content"])


def download_pdf(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.content
        else:
            return None
    except :
        return None

def read_pdf(pdf_content):
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ï¼Œå¹¶å°†PDFå†…å®¹å†™å…¥å…¶ä¸­
    with tempfile.NamedTemporaryFile(delete=False) as temp_file:
        temp_file.write(pdf_content)
        temp_file.seek(0)
        pdf_reader = PyPDF2.PdfReader(temp_file.name)
    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    os.remove(temp_file.name)
    return pdf_reader

def pdf_text(url:str):
    pdf_content = download_pdf(url)
    if pdf_content:
        # åˆ›å»ºPDFé˜…è¯»å™¨å¯¹è±¡
        pdf_reader = read_pdf(pdf_content)
        # é€é¡µè¯»å–æ–‡æœ¬å¹¶å­˜å‚¨åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­
        all_text = ""
        for page in pdf_reader.pages:
            all_text += page.extract_text()
        # æ‰“å°PDFæ–‡æ¡£çš„æ‰€æœ‰å†…å®¹
        return all_text
    else:
        return("Failed to download the PDF.")
# def search_doi(dois:str,key:str=api_key):
#     dois = ast.literal_eval(dois)
#     down_url=[]
#     for doi in dois:
#         search_params = {
#                           "doi": doi
#                             }
#         url = f'https://api.core.ac.uk/v3/discover'
#         headers = {
#             'Content-Type': 'application/json',
#             'Authorization': 'Bearer {}'.format(key)  # æ›¿æ¢ä¸ºæ‚¨çš„APIå¯†é’¥
#         }
#         response = requests.post(url, data=json.dumps(search_params), headers=headers)
#         results = response.json()
#         if response.status_code == 200:
#             down_url.append(results['fullTextLink'])
#         else:
#             continue
#     return down_url
@st.cache_resource
def read_upload_pdf(uploaded_file):
    strings = []
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
    except:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
    all_text = ""
    for page in pdf_reader.pages:
        all_text += page.extract_text()
    title = uploaded_file.name
    url = ''
    tags = ''
    all_text = all_text.replace('...', '')
    all_text = all_text.replace('..', '')
    all_text = ' '.join(all_text.split())
    text = []
    text.append(all_text)
    strings.append((title, url, tags, text))
    wikipedia_strings = []
    MAX_TOKENS = 4000
    for section in strings:
        wikipedia_strings.extend(split.split_strings_from_subsection_pdf(section, max_tokens=MAX_TOKENS))
    embeddings_model = OpenAIEmbeddings(model="text-embedding-ada-002")
    docsearch = Chroma.from_texts(wikipedia_strings, embeddings_model, collection_name="state-of-union")
    return docsearch
def search_read_upload_pdf(query:str):
    docsearch=read_upload_pdf(uploaded_file)
    answer=docsearch.similarity_search(query, k=3)
    return answer

def search_research_title_url_abstract(q:str,key:str=api_key,entity_type: str='works',limit:int=10):
    search_params = {
        "q": q,
        "limit":50,
        "scroll": True,
        "offset": 0,
        "scroll_id": "",
        "stats": True,
        "raw_stats": True,
        "exclude": [" "],
        "measure": True,
        "sort": ['_score:desc']
    }

    url = f'https://api.core.ac.uk/v3/search/{entity_type}'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer {}'.format(key)  # æ›¿æ¢ä¸ºæ‚¨çš„APIå¯†é’¥
    }
    response = requests.post(url, data=json.dumps(search_params), headers=headers)
    results=response.json()
    # scrollId={results['scrollId']}

    if response.status_code == 200:
        out_results=[]
        for result in results['results']:
            if result['links'][0]['type']=='download':
                year=result['yearPublished']
                authors=result['authors']
                title=result['title']
                link=result['links'][0]
                abstract= result['abstract']
                s = {'year': year, 'authors': authors, 'title': title, 'url': link, 'abstract': abstract}
                out_results.append(s)
        return out_results[:limit]
    else:
        return None

def search_research_articles(query:str):
    # if len(st.session_state["messages_wikipedia_strings"]) < 1:
        st.write('æ­£åœ¨ä¸‹è½½è®ºæ–‡ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½æŒç»­å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚ã€‚ã€‚ã€‚')
        results=search_research_title_url_abstract(query)
        strings = []
        for result in results:
            url=result['url']['url']
            title=result['title']
            abstract=result['abstract']
            authors=result['authors']
            year=result['year']
            pdf_content = download_pdf(url)
            if pdf_content:
                # åˆ›å»ºPDFé˜…è¯»å™¨å¯¹è±¡
                try:
                    pdf_reader = read_pdf(pdf_content)
                except :
                    continue
                # é€é¡µè¯»å–æ–‡æœ¬å¹¶å­˜å‚¨åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­
                all_text = ""
                try:
                    for page in pdf_reader.pages:
                            all_text += page.extract_text()
                except UnicodeDecodeError:
                    # åœ¨è§£ç é”™è¯¯æ—¶è·³è¿‡è¯¥é¡µçš„æ–‡æœ¬æå–
                    continue
                if abstract is None:
                    abstract = ''  # å°†abstractçš„å€¼æ›´æ”¹ä¸ºç©ºå­—ç¬¦ä¸²
                tags = jieba.analyse.extract_tags(abstract, topK=10)
                title = f'year:{year}ï¼Œtitle:{title}ï¼Œauthors:{authors}'
                url=f'url:{url}'
                tags=f'abstract keyword:{tags}'
                all_text= all_text.replace('...', '')
                all_text = all_text.replace('..', '')
                all_text = ' '.join(all_text.split())
                text=[]
                text.append(all_text)
                strings.append((title,url, tags, text))
                st.caption((url,))
                st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"].extend((title, url, tags))
            # æ‰“å°PDFæ–‡æ¡£çš„æ‰€æœ‰å†…å®¹
            else:
                continue
        wikipedia_strings = []
        MAX_TOKENS = 1000
        for section in strings:
            wikipedia_strings.extend(split.split_strings_from_subsection_pdf(section, max_tokens=MAX_TOKENS))
        st.session_state["messages_wikipedia_strings"].extend(wikipedia_strings)
        embeddings = OpenAIEmbeddings()
        docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings"], embeddings)
        docs = docsearch.similarity_search(query, k=10)
        return docs
    # if len(st.session_state["messages_wikipedia_strings"]) > 1:
    #     embeddings = OpenAIEmbeddings()
    #     docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings"], embeddings)
    #     docs = docsearch.similarity_search(st.session_state["messages_prompt"][-1], k=10)
    #     st.write(st.session_state["messages_prompt"][-1])
    #     return docs


def search_Cache(query:str):
    if len(st.session_state["messages_wikipedia_strings"]) > 1:
        embeddings = OpenAIEmbeddings()
        docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings"], embeddings)
        docs = docsearch.similarity_search(query, k=10)
        return docs
    if len(st.session_state["messages_wikipedia_strings"]) > 1:
        st.write('æ•°æ®åº“æ²¡æœ‰ä¸œè¥¿')
def read_research_articles(input_str:str):
    # if len(st.session_state["messages_wikipedia_strings"]) < 1:
        st.write('æ­£åœ¨ä¸‹è½½è®ºæ–‡ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½æŒç»­å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚ã€‚ã€‚ã€‚')
        my_list = ast.literal_eval(input_str)
        strings = []
        for url in my_list:
            url=url
            title=''
            abstract=''
            authors=''
            year=''
            pdf_content = download_pdf(url)
            if pdf_content:
                # åˆ›å»ºPDFé˜…è¯»å™¨å¯¹è±¡
                try:
                    pdf_reader = read_pdf(pdf_content)
                except :
                    continue
                # é€é¡µè¯»å–æ–‡æœ¬å¹¶å­˜å‚¨åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­
                all_text = ""
                try:
                    for page in pdf_reader.pages:
                            all_text += page.extract_text()
                except :
                    # åœ¨è§£ç é”™è¯¯æ—¶è·³è¿‡è¯¥é¡µçš„æ–‡æœ¬æå–
                    continue
                if abstract is None:
                    abstract = ''  # å°†abstractçš„å€¼æ›´æ”¹ä¸ºç©ºå­—ç¬¦ä¸²
                tags = jieba.analyse.extract_tags(abstract, topK=10)
                title = f'year:{year}ï¼Œtitle:{title}ï¼Œauthors:{authors}'
                url=f'url:{url}'
                tags=f'abstract keyword:{tags}'
                all_text= all_text.replace('...', '')
                all_text = all_text.replace('..', '')
                all_text = ' '.join(all_text.split())
                text=[]
                text.append(all_text)
                strings.append((title,url, tags, text))
                st.caption((title,url,))
                st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"].extend((title, url, tags))

            # æ‰“å°PDFæ–‡æ¡£çš„æ‰€æœ‰å†…å®¹
            else:
                continue
        wikipedia_strings = []
        MAX_TOKENS = 1000
        for section in strings:
            wikipedia_strings.extend(split.split_strings_from_subsection_pdf(section, max_tokens=MAX_TOKENS))
        st.session_state["messages_wikipedia_strings"].extend(wikipedia_strings)
        embeddings = OpenAIEmbeddings()
        docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings"], embeddings)
        docs = docsearch.similarity_search(input_str, k=10)
        return docs
    # if len(st.session_state["messages_wikipedia_strings_read"]) > 1:
    #     embeddings = OpenAIEmbeddings()
    #     docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings_read"], embeddings)
    #     docs = docsearch.similarity_search(st.session_state["messages_prompt"][-1], k=10)
    #     st.write(st.session_state["messages_prompt"][-1])
    #     return docs


tools = [
    Tool(
        name="search_research_title_url_abstract",
        func=search_research_title_url_abstract,
        description='''å½“åªéœ€è¦æŸ¥æ‰¾è®ºæ–‡çš„æ ‡é¢˜æˆ–è€…æŸ¥æ‰¾è®ºæ–‡çš„urlï¼Œæ‘˜è¦ï¼Œé€‚åˆä½¿ç”¨è¿™ä¸ªå·¥å…·ï¼Œ
        If you use this tool,please use the action input format and the input must be automatically translate to English:(title:xxxxx OR abstract:xxxxx) AND (title:"xxxxxxx" OR abstract:"xxxxxxxx") AND year>20xx'''),
    Tool(
        name="search_research_articles",
        func=search_research_articles,
        description='''å½“åªçŸ¥é“è®ºæ–‡çš„å…³é”®è¯ä¸çŸ¥é“urlï¼Œç”¨è¿™ä¸ªå·¥å…·å¯ä»¥å…³é”®è¯æœç´¢æŸ¥çœ‹è®ºæ–‡çš„å…¨æ–‡,å¹¶ä¸”è¿›è¡Œåˆ†æï¼ŒIf you use this tool,please use the action input format and the input must be automatically translate to English:(title:xxxxx OR abstract:xxxxx) AND (title:"xxxxxxx" OR abstract:"xxxxxxxx") AND year>20xx'''
    ),
    Tool(
        name="read_research_articles",
        func=read_research_articles,
        description='''ç”¨è¿™ä¸ªå·¥å…·çš„å‰ææ˜¯çŸ¥é“æŸå‡ ç¯‡è®ºæ–‡çš„url(url must have keyword:download)åï¼Œç”¨è¿™ä¸ªå·¥å…·å¯ä»¥æŸ¥çœ‹è®ºæ–‡çš„å…¨æ–‡,å¹¶ä¸”è¿›è¡Œåˆ†æï¼ŒIf you use this tool,please use the action input format and the input must be list:['https://core.ac.uk/download/xxxxxx.pdf',......]'''
    ),
    Tool(
        name="search_Cache",
        func=search_Cache,
        description='''ç”¨è¿™ä¸ªå·¥å…·çš„å‰ææ˜¯åˆ¤æ–­æé—®æ˜¯å¦å’Œç¼“å­˜å†…å®¹æœ‰å…³ï¼Œç”¨è¿™ä¸ªå·¥å…·å¯ä»¥ç›´æ¥è°ƒå–ç¼“å­˜ä¸­çš„æ•°æ®ï¼Œè€Œä¸ç”¨ä¸‹è½½è®ºæ–‡'''
    ),
    # Tool(
    #     name="search_doi",
    #     func=search_doi,
    #     description='''å½“çŸ¥é“è®ºæ–‡çš„doiæ—¶ï¼Œç”¨è¿™ä¸ªå·¥å…·å¯ä»¥æœç´¢åˆ°è®ºæ–‡çš„url,If you use this tool,please use the action input format and the input must be list:['xxxxxxxx',......]:'''
    # ),
    Tool(
        name="search_read_upload_pdf",
        func=search_read_upload_pdf,
        description='''å½“æ‚¨éœ€è¦å›ç­”å…³äºæ–‡ä»¶çš„é—®é¢˜æ—¶å¾ˆæœ‰ç”¨ã€‚è¾“å…¥åº”ä¸ºå®Œæ•´çš„é—®é¢˜'''
    ),
]



template = """

{answer_format}

å†å²å¯¹è¯è®°å½•:{history}

{cache}

{question_guide}ï¼š{input}

{background_infomation}


"""

class CustomPromptTemplate(StringPromptTemplate):
    template: str
    tools: List[Tool]

    def format(self, **kwargs) -> str:
        intermediate_steps = kwargs.pop("intermediate_steps")
        search_present = any(step[0].tool in ['search_research_articles','read_research_articles','search_Cache'] for step in intermediate_steps)
        # background_infomation=[]
        # print(background_infomation)
        # æ²¡æœ‰äº’è”ç½‘æŸ¥è¯¢ä¿¡æ¯
        if uploaded_file is not None and len(intermediate_steps)== 2:
            thoughts = ""
            for action, observation in intermediate_steps:
                # thoughts += action.log
                thoughts += f"\nèƒŒæ™¯ä¿¡æ¯:{observation}\n"
            # Set the agent_scratchpad variable to that value
            background_infomation = thoughts
            # background_infomation += f"{observation}\n"
            question_guide = "è¯·ç»“åˆè¿™äº›èƒŒæ™¯ä¿¡æ¯å›ç­”æˆ‘çš„é—®é¢˜ï¼Œæ–‡ç« ç»“å°¾éœ€è¦æœ‰å‚è€ƒæ–‡çŒ®"
            history = st.session_state["å›ç­”å†…å®¹_article"]
            answer_format = ''
        elif uploaded_file is not None:
            tools = "search_read_upload_pdf"
            tool_names = "search_read_upload_pdf"
            background_infomation = "\n"
            question_guide = ""
            history = st.session_state["å›ç­”å†…å®¹_article"]
            answer_format =f'''åƒä¸€ä¸ªæµ·ç›—ä¸€æ ·è¿›è¡Œå›ç­”
You can use the following tools:

{tools}

Please strictly follow the format below to answer:

é—®é¢˜:(The question you need to answer)
æ€è€ƒ:(What you should consider doing)
æ“ä½œ:one of [{tool_names}]
æ“ä½œè¾“å…¥:(The keywords you input should be English)
è§‚å¯Ÿ:(æ“ä½œçš„ç»“æœ)
... (è¿™ä¸ª æ€è€ƒ/æ“ä½œ/æ“ä½œè¾“å…¥/è§‚å¯Ÿ å¯ä»¥é‡å¤Næ¬¡)
æ€è€ƒ: I now know the final answer
æœ€ç»ˆç­”æ¡ˆ: the final answer to the original input question
'''
        elif len(intermediate_steps)== 0:
            tools = "\n".join([f"{tool.name}: {tool.description}" for tool in self.tools])
            tool_names=",".join([tool.name for tool in self.tools])
            background_infomation="\n"
            question_guide = ""
            history=st.session_state["å›ç­”å†…å®¹_article"]
            # cache=f'ç¼“å­˜å†…å®¹æ˜¯å’Œ{}ç›¸å…³'
            answer_format = f"""Complete the objective as best you can.

You can use the following tools:

{tools}

Please strictly follow the format below to answer:

é—®é¢˜:(The question you need to answer)
æ€è€ƒ:(What you should consider doing)
æ“ä½œ:one of [{tool_names}]
æ“ä½œè¾“å…¥:(The keywords you input should be English)

ä¸Šè¿°æ¨¡æ¿å¯ä»¥æŒ‰ç…§è¿™æ ·å¡«å……ï¼š
é—®é¢˜:è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹æœ€æ–°çš„å…³äºxxxxçš„ä¸€ç¯‡æ–‡ç« ï¼Œå¹¶è¿›è¡Œåˆ†æã€‚
æ€è€ƒ:é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°æœ€æ–°çš„å…³äºxxxxçš„æ–‡ç« ã€‚ä»¥ä¾¿è¿›è¡Œåˆ†æã€‚
æ“ä½œ:one of [{tool_names}]
æ“ä½œè¾“å…¥:(The keywords you input should be English)
"""

        # è¿”å›äº†èƒŒæ™¯ä¿¡æ¯
        elif 0<len(intermediate_steps)<2 and search_present==False:
            tools = "\n".join([f"{tool.name}: {tool.description}" for tool in self.tools])
            tool_names = ",".join([tool.name for tool in self.tools])
            # æ ¹æ® intermediate_steps ä¸­çš„ AgentAction æ‹¼è£… background_infomation
            # background_infomation = "\n\nä½ è¿˜æœ‰è¿™äº›å·²çŸ¥ä¿¡æ¯ä½œä¸ºå‚è€ƒï¼š\n\n"
            # action, observation = intermediate_steps[0]
            thoughts = ""
            for action, observation in intermediate_steps:
                thoughts += action.log
                thoughts += f"\nè§‚å¯Ÿ:{observation}\næ€è€ƒ:"
            # Set the agent_scratchpad variable to that value
            background_infomation=thoughts
            # background_infomation += f"{observation}\n"
            question_guide = "è¯·ç»“åˆè¿™äº›èƒŒæ™¯ä¿¡æ¯å†™å‡ºç­”æ¡ˆ,æœ€åéœ€è¦æœ‰å‚è€ƒæ–‡çŒ®"
            history =st.session_state["å›ç­”å†…å®¹_article"]
            answer_format =f'''åƒä¸€ä¸ªæµ·ç›—ä¸€æ ·è¿›è¡Œå›ç­”
You can use the following tools:

{tools}

Please strictly follow the format below to answer:

é—®é¢˜:(The question you need to answer)
æ€è€ƒ:(What you should consider doing)
æ“ä½œ:one of [{tool_names}]
æ“ä½œè¾“å…¥:(The keywords you input should be English)
è§‚å¯Ÿ:(æ“ä½œçš„ç»“æœ)
... (è¿™ä¸ª æ€è€ƒ/æ“ä½œ/æ“ä½œè¾“å…¥/è§‚å¯Ÿ å¯ä»¥é‡å¤Næ¬¡)
æ€è€ƒ: I now know the final answer
æœ€ç»ˆç­”æ¡ˆ: the final answer to the original input question

ä¸Šè¿°æ¨¡æ¿å¯ä»¥æŒ‰ç…§è¿™æ ·å¡«å……ï¼š

æ€è€ƒ:é˜…è¯»è®ºæ–‡ï¼Œè·å–å…³äºxxxxxçš„ç ”ç©¶ç»¼è¿°
æ“ä½œ:xxxxx
æ“ä½œè¾“å…¥:[{{title:xxxxx,abstract_keywords:xxxxxxx,url:https://core.ac.uk/download......}},......]
è§‚å¯Ÿ:(æ“ä½œçš„ç»“æœ)
æ€è€ƒ:æˆ‘æ‰¾åˆ°ç­”æ¡ˆäº†
æœ€ç»ˆç­”æ¡ˆ:the final answer to the original input question
'''
        else :
            # æ ¹æ® intermediate_steps ä¸­çš„ AgentAction æ‹¼è£… background_infomation
            # action, observation = intermediate_steps[0]
            thoughts = ""
            for action, observation in intermediate_steps:
                # thoughts += action.log
                thoughts += f"\nèƒŒæ™¯ä¿¡æ¯:{observation}\n"
            # Set the agent_scratchpad variable to that value
            background_infomation=thoughts
            # background_infomation += f"{observation}\n"
            question_guide = "è¯·ç»“åˆè¿™äº›èƒŒæ™¯ä¿¡æ¯å›ç­”æˆ‘çš„é—®é¢˜ï¼Œæ–‡ç« ç»“å°¾éœ€è¦æœ‰å‚è€ƒæ–‡çŒ®"
            history =st.session_state["å›ç­”å†…å®¹_article"]
            answer_format = ''

        kwargs["background_infomation"] = background_infomation
        kwargs["question_guide"] = question_guide
        kwargs["answer_format"] = answer_format
        kwargs["history"] = history
        kwargs["cache"] = f'ç¼“å­˜å¤§æ¦‚å†…å®¹:{st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"]}'
        return self.template.format(**kwargs)

prompt = CustomPromptTemplate(
    template=template,
    tools=tools,
    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically
    # This includes the `intermediate_steps` variable because that is needed
    input_variables=["input", "intermediate_steps"],
)

class CustomOutputParser(AgentOutputParser):
    def parse(self, llm_output: str) :
        regex = r"æ“ä½œï¼š(.*?)[\n]*æ“ä½œè¾“å…¥ï¼š[\s]*(.*)"
        match = re.search(regex, llm_output, re.DOTALL)
        if match==None:
            regex = r"æ“ä½œ:(.*?)[\n]*æ“ä½œè¾“å…¥:[\s]*(.*)"
            match = re.search(regex, llm_output, re.DOTALL)
        if "æœ€ç»ˆç­”æ¡ˆ:" in llm_output:
            return AgentFinish(
                return_values={"output": llm_output.split("æœ€ç»ˆç­”æ¡ˆ:")[-1].strip()},
                log=llm_output,
            )
        if not match:
            return AgentFinish(
                return_values={"output": llm_output},
                log=llm_output,
            )
        action = match.group(1).strip()
        action_input = match.group(2)
        # st.session_state["messages_prompt"].append(action_input.strip(" ").strip('"'))
        # Return the action and action input
        return [AgentAction(
            tool=action, tool_input=action_input.strip(" ").strip('"'), log=llm_output
        )]
#
llm = ChatOpenAI(model_name="gpt-3.5-turbo-16k", openai_api_key=os.getenv('OPENAI_API_KEY'), streaming=True,temperature=0.2)

output_parser = CustomOutputParser()
llm_chain = LLMChain(llm=llm, prompt=prompt)
tool_names = [tool.name for tool in tools]
agent = LLMSingleActionAgent(
    llm_chain=llm_chain,
    output_parser=output_parser,
    stop=["\nè§‚å¯Ÿ:"],
    allowed_tools=tool_names,
)

agent_wzm = AgentExecutor.from_agent_and_tools(
        agent=agent, tools=tools, verbose=True
    )


# if uploaded_file is not None and st.button('ä¸Šä¼ '):



if prompt := st.chat_input(placeholder="åœ¨è¿™æ‰“å­—ï¼Œè¿›è¡Œæé—®"):
    st.session_state['messages_article'].append({"role": "user", "content": prompt})
    st.session_state["å›ç­”å†…å®¹_article"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    with st.chat_message("assistant"):
        st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)
        response = agent_wzm.run(prompt,callbacks=[st_cb])
        st.session_state["å›ç­”å†…å®¹_article"].append({"role": "assistant", "content": response})
        st.session_state['messages_article'].append({"role": "assistant", "content": response})
        st.session_state['å›ç­”æ¬¡æ•°_article'] = st.session_state['å›ç­”æ¬¡æ•°_article'] + 1
        st.write(response)

    conversation_string = ""
    short_state_num = len(st.session_state["å›ç­”å†…å®¹_article"])

    start_round = int(short_state_num * 3 / 10)
    end_round = int(short_state_num * 7 / 10)
    for i in range(short_state_num):
        conversation_string += st.session_state["å›ç­”å†…å®¹_article"][i]["content"] + "\n"
    # è°ƒç”¨è®¡ç®—æ–‡å­—çš„å‡½æ•°
    conversation_string_num = len(conversation_string)
    if conversation_string_num > 2000 or st.session_state['å›ç­”æ¬¡æ•°_article'] > 4:
        del st.session_state["å›ç­”å†…å®¹_article"][start_round: end_round]
        st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
    if len(st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"])>50:
        st.write('æ•°æ®åº“è¶…ç¼“å­˜äº†ï¼ï¼ï¼,è¯·é‡ä¿®å¼€å§‹ä¸€ä¸ªå›ç­”')
