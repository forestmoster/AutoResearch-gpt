# if your deploy app in local you should not use it
import sys
__import__("pysqlite3")
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")


from langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent
import shutil
import sys
import uuid
import pandas as pd
from langchain.agents import AgentExecutor
from langchain.callbacks import StreamlitCallbackHandler
from langchain.schema import BaseLanguageModel
import streamlit as st
from Web_Chroma import WebChroma, streamlit_sidebar_delete_database
import os
from typing import Type
from pydantic import BaseModel, Field
from langchain.tools import BaseTool
from langchain.agents import AgentType
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent
from FILE_Chroma import FileChroma

if 'session_id' not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())  # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„UUID
session_folder = os.path.join('tmp', st.session_state.session_id)
vector_folder_web = os.path.join(session_folder, 'vector_web')
catch_picture = os.path.join(session_folder, 'catch_picture')
if not os.path.exists(vector_folder_web):
    os.makedirs(vector_folder_web)
if not os.path.exists(session_folder):
    os.makedirs(session_folder)
if not os.path.exists(catch_picture):
    os.makedirs(catch_picture)
random_file_name = str(uuid.uuid4())
chromaweb = WebChroma(vector_folder_web)
chromfile = FileChroma(vector_folder_web)


def search_research_articles(query, search_query):
    return chromaweb.search_research_articles(query, search_query)


def read_research_articles(query, search_query: str):
    return chromaweb.read_research_articles(query, search_query)


def search_Cache(query: str):
    return chromaweb.search_Cache(query)


def search_research_title_url_abstract(query: str):
    return chromaweb.search_research_title_url_abstract(query)


def search_doi(dois: str):
    chromaweb.search_doi(dois)


@st.cache_resource
def read_upload_pdf(uploaded_files):
    return chromfile.upload_pdfs_chroma_catch(uploaded_files)


def search_read_upload_pdf(query: str):
    docsearch = read_upload_pdf(uploaded_file)
    answer = docsearch.similarity_search(query, k=3)
    return answer


styl = """
<style>

    .stButton{
        position: fixed;
        bottom: 1rem;
        left:500;
        right:500;
        z-index:999;
    }

    @media screen and (max-width: 1000px) {

        .stButton {
            left:2%;
            width: 100%;
            bottom:1.1rem;
            z-index:999;
        }
    }

</style>

"""

st.markdown(styl, unsafe_allow_html=True)
# st.set_page_config(layout="wide")
st.title("ğŸ’¬ å¤–æ–‡æ–‡çŒ®åŠ©æ‰‹+ä¸ç‰ˆæ•°æ®åˆ†æåŠ©æ‰‹")
st.caption(
    'ä½ å¯ä»¥è”ç½‘æŸ¥è¯¢ç›¸å…³é¢†åŸŸçš„å¤–æ–‡æ–‡çŒ®ï¼Œå¹¶åšåˆæ­¥çš„åˆ†æã€‚ä½ ä¹Ÿå¯ä»¥ä¸Šä¼ ä¸€ä¸ªpdfè¿›è¡Œæ–‡æœ¬åˆ†ææˆ–è€…csvè¿›è¡Œæ•°æ®åˆ†æï¼Œç»˜åˆ¶ç§‘ç ”å›¾çº¸')
uploaded_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ªçº¯æ–‡æœ¬docxæ–‡ä»¶æˆ–è€…pdfæ–‡ä»¶", accept_multiple_files=False,
                                 label_visibility="hidden")
if uploaded_file:
    uploaded_file = [uploaded_file]
if uploaded_file is None:
    st.cache_resource.clear()
if "messages_article" not in st.session_state:
    st.session_state["messages_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
if "å›ç­”å†…å®¹_article" not in st.session_state:
    st.session_state["å›ç­”å†…å®¹_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
if 'å›ç­”æ¬¡æ•°_article' not in st.session_state:
    st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
if "messages_wikipedia_ç¼“å­˜å…³é”®è¯" not in st.session_state:
    st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"] = []

if st.button('é‡æ–°å¼€å§‹ä¸€ä¸ªå›ç­”'):
    del st.session_state["å›ç­”å†…å®¹_article"]
    del st.session_state["messages_article"]
    del st.session_state["å›ç­”æ¬¡æ•°_article"]
    del st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"]
    st.session_state["å›ç­”å†…å®¹_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
    st.session_state["messages_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
    st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
    st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"] = []
    chromaweb.delete_all_vector_database()
    # æ¸…ç©ºæ–‡æœ¬è¾“å…¥æ¡†çš„å†…å®¹
    user_input = ""

for msg in st.session_state["messages_article"]:
    st.chat_message(msg["role"]).write(msg["content"])


def csv_agent(
        llm: BaseLanguageModel,
        uploaded_files: uploaded_file,
) -> AgentExecutor:
    # æ£€æŸ¥ uploaded_files ä¸­çš„ CSV æ–‡ä»¶æ•°é‡
    csv_files = [f for f in uploaded_files if f.name.lower().endswith('.csv')]
    if len(csv_files) != 1:
        raise ValueError("è¯·ç¡®ä¿ä¸Šä¼ äº†ä¸€ä¸ªä¸”åªæœ‰ä¸€ä¸ª CSV æ–‡ä»¶!")
    df = pd.read_csv(csv_files[0])  # ä½¿ç”¨ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰CSVæ–‡ä»¶
    return create_pandas_dataframe_agent(llm=llm, df=df, verbose=True, return_intermediate_steps=True)


# xxxxxxxxxxxxxxxxxxxxxxxxxTOOLSxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
class search_research_title_url_abstract_input(BaseModel):
    """Inputs for get_current_stock_price"""
    query: str = Field(
        description='''the input must be automatically translate to English:(title:xxxxx OR abstract:xxxxx) AND (title:'xxxxxxx" OR abstract:"xxxxxxxx") AND year>20xx''')


class search_research_title_url_abstract_tool(BaseTool):
    name = "search_research_title_url_abstract"
    description = "å½“åªéœ€è¦æŸ¥æ‰¾è®ºæ–‡çš„æ ‡é¢˜æˆ–è€…æŸ¥æ‰¾è®ºæ–‡çš„urlï¼Œæ‘˜è¦ï¼Œé€‚åˆä½¿ç”¨è¿™ä¸ªå·¥å…·"
    args_schema: Type[BaseModel] = search_research_title_url_abstract_input

    def _run(self, query: str):
        response = search_research_title_url_abstract(query)
        return response

    def _arun(self, query: str):
        raise NotImplementedError("search_research_title_url_abstract does not support async")


class read_research_articles_Input(BaseModel):
    query: str = Field(
        description='''You must provide URLs. Use the following format: ["https://core.ac.uk/download/xxxxxx.pdf", ...]      Note: The query must be a str.''')
    search_query: str = Field(
        escription="Enter the question you want to search for in research articles. Cannot be empty.")


class read_research_articles_Tool(BaseTool):
    name = "read_research_articles"
    description = "if you have urls ,you can use this to download and read articles"
    args_schema: Type[BaseModel] = read_research_articles_Input

    def _run(self, query: str, search_query: str):
        response = read_research_articles(query, search_query)
        return response

    def _arun(self, query: str, search_query: str):
        raise NotImplementedError("read_research_articles does not support async")


class search_doi_input(BaseModel):
    """Inputs for get_current_stock_price"""
    dois: str = Field(
        description='''You must provide dois. Use the following format: ["'xxxxxxxx',......]  Note: The query must be a list.''')


class search_doi_tool(BaseTool):
    name = "search_doi"
    description = "if you have doi ,you can use this to read articles"
    args_schema: Type[BaseModel] = search_doi_input

    def _run(self, dois: str):
        response = search_doi(dois)
        return response

    def _arun(self, query: str):
        raise NotImplementedError("search_doi does not support async")


class search_Cache_input(BaseModel):
    """Inputs for get_current_stock_price"""
    query: str = Field(description='''you need provide query to search content to answer question ''')


class search_Cache_tool(BaseTool):
    name = "search_Cache"
    description = "Use this tool to determine if your query is related to indexed content in the cache. Retrieve data directly from the cache without needing to download entire papers."
    args_schema: Type[BaseModel] = search_Cache_input

    def _run(self, query: str):
        response = search_Cache(query)
        return response

    def _arun(self, query: str):
        raise NotImplementedError("search_Cache does not support async")


class search_read_upload_pdf_input(BaseModel):
    """Inputs for get_current_stock_price"""
    query: str = Field(description='''you need provide a complete query to search content to answer question ''')


class search_read_upload_pdf_tool(BaseTool):
    name = "search_read_upload_pdf"
    description = "Use this tool to answer question with upload_pdf"
    args_schema: Type[BaseModel] = search_read_upload_pdf_input

    def _run(self, query: str):
        response = search_read_upload_pdf(query)
        return response

    def _arun(self, query: str):
        raise NotImplementedError("search_read_upload_pdf does not support async")


# xxxxxxxxxxxxxxxxxxxxxxxxxTOOLSxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx


llm = ChatOpenAI(model="gpt-3.5-turbo-16k-0613", temperature=0)
tools = [search_research_title_url_abstract_tool(), read_research_articles_Tool(), search_doi_tool(),
         search_Cache_tool()]
agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)
tools_pdf = [search_read_upload_pdf_tool()]
agent_pdf = initialize_agent(tools=tools_pdf, llm=llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)

accepted_extensions = ('.csv',)
if prompt := st.chat_input(placeholder="åœ¨è¿™æ‰“å­—ï¼Œè¿›è¡Œæé—®"):
    # æ¸…é™¤ç¼“å­˜å›¾ç‰‡clean_tmp
    if prompt == 'clean':
        work_directory = "./tmp"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„å·¥ä½œç›®å½•è·¯å¾„
        # éå†å·¥ä½œæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for filename in os.listdir(work_directory):
            file_path = os.path.join(work_directory, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)  # åˆ é™¤æ–‡ä»¶
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)  # åˆ é™¤ç›®å½•
            except Exception as e:
                print(f"Failed to delete {file_path}. Reason: {e}")
        st.write("All file in the working directory have been deleted.")
        sys.exit()

    st.chat_message("user").write(prompt)
    with st.chat_message("assistant"):
        if uploaded_file:
            csv_file_name_csv = next((f.name for f in uploaded_file if f.name.lower().endswith('.csv')), None)
            csv_file_name_pdf = next((f.name for f in uploaded_file if f.name.lower().endswith('.pdf')), None)
            # if uploaded_file is not None and uploaded_file.name.lower().endswith(accepted_extensions):
            if csv_file_name_csv:
                agent_wzm = csv_agent(llm=ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613"),
                                      uploaded_files=uploaded_file)
                st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)
                response_orgin = agent_wzm(f'''question:{prompt},
                        history:{st.session_state['å›ç­”å†…å®¹_article']},

            Whenever you're generating or modifying a plot, you must display it on Streamlit. The process involves the following steps:

            1. First, save the image to a temporary directory. You can use the following command: 'plt.savefig('./tmp/{st.session_state.session_id}/catch_picture/{random_file_name}.png')'.
            2. Second, ensure you have already imported Streamlit with 'import streamlit as st' at the beginning of your code.
            3. Third, after saving the image, you can display it on your Streamlit app using: 'st.image('./tmp/{st.session_state.session_id}/catch_picture/{random_file_name}.png')'.
            4. Finally, after it's been displayed, delete the image using: 'os.remove('./tmp/{st.session_state.session_id}/catch_picture/{random_file_name}.png')'.

            Remember, all plots should have a clear title and axis labels for better interpretation.

            you must use this 'Action:python_repl_ast' ''', callbacks=[st_cb])

                for observersion in response_orgin["intermediate_steps"]:
                    try:
                        if isinstance(observersion[1], pd.DataFrame):
                            st.dataframe(observersion[1])
                    except TypeError as e:
                        pass
                response = response_orgin['output']
            # å¦åˆ™è¿è¡Œè¿™ä¸ªagent
            elif csv_file_name_pdf:
                st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)
                response = agent_pdf.run(f'''history:{st.session_state['å›ç­”å†…å®¹_article']},
                question:{prompt},analyse the upload file ''', callbacks=[st_cb])
        else:
            st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)
            response = agent.run(f'''history:{st.session_state['å›ç­”å†…å®¹_article']},
            question:{prompt},''', callbacks=[st_cb])

        # å‰ç«¯ç¼“å­˜å¤„ç†
        st.session_state['messages_article'].append({"role": "user", "content": prompt})
        st.session_state["å›ç­”å†…å®¹_article"].append({"role": "user", "content": prompt})
        st.session_state["å›ç­”å†…å®¹_article"].append({"role": "assistant", "content": response})
        st.session_state['messages_article'].append({"role": "assistant", "content": response})
        st.session_state['å›ç­”æ¬¡æ•°_article'] = st.session_state['å›ç­”æ¬¡æ•°_article'] + 1
        st.write(response)

    conversation_string = ""
    short_state_num = len(st.session_state["å›ç­”å†…å®¹_article"])
    start_round = int(short_state_num * 3 / 10)
    end_round = int(short_state_num * 7 / 10)
    for i in range(short_state_num):
        conversation_string += st.session_state["å›ç­”å†…å®¹_article"][i]["content"] + "\n"
    # è°ƒç”¨è®¡ç®—æ–‡å­—çš„å‡½æ•°
    conversation_string_num = len(conversation_string)
    if conversation_string_num > 2000 or st.session_state['å›ç­”æ¬¡æ•°_article'] > 4:
        del st.session_state["å›ç­”å†…å®¹_article"][start_round: end_round]
        st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
    if len(st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"]) > 50:
        st.write('æ•°æ®åº“è¶…ç¼“å­˜äº†ï¼ï¼ï¼,è¯·é‡ä¿®å¼€å§‹ä¸€ä¸ªå›ç­”')

streamlit_sidebar_delete_database(chromaweb)