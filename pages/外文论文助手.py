
# import sys
# __import__("pysqlite3")
# sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")


import ast
import sys
from typing import List
import re
import jieba.analyse
import openai
import pandas as pd
from langchain import LLMChain, OpenAI, PromptTemplate, text_splitter
from langchain.agents import AgentOutputParser, LLMSingleActionAgent, AgentExecutor, initialize_agent, AgentType
from langchain.agents.agent_toolkits import create_python_agent
from langchain.callbacks import StreamlitCallbackHandler
from langchain.callbacks.tracers import langchain
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import StringPromptTemplate
from langchain.schema import AgentFinish, AgentAction, BaseLanguageModel
from langchain.text_splitter import CharacterTextSplitter
from langchain.tools import Tool, PythonREPLTool
import streamlit as st
from langchain.vectorstores import Chroma
import PyPDF2
import tempfile
import split
import requests
import json
api_key='8j9mqPr37oHsORDKJTWyeYMdBGgA5cZz'
import os

def list_directory_contents(directory_path):
    return os.listdir(directory_path)

def on_file_change(file):
    # åœ¨è¿™é‡Œå¤„ç†æ–‡ä»¶ä¸Šä¼ åçš„æ“ä½œ
    return f'æ–‡ä»¶å: {file.name},æ–‡ä»¶å¤§å°: {file.size} bytes'
styl = """
<style>

    .stButton{
        position: fixed;
        bottom: 1rem;
        left:500;
        right:500;
        z-index:999;
    }

    @media screen and (max-width: 1000px) {

        .stButton {
            left:2%;
            width: 100%;
            bottom:1.1rem;
            z-index:999;
        }
    }

</style>

"""

st.markdown(styl, unsafe_allow_html=True)
# st.set_page_config(layout="wide")
st.title("ğŸ’¬ å¤–æ–‡æ–‡çŒ®åŠ©æ‰‹+ä¸ç‰ˆæ•°æ®åˆ†æåŠ©æ‰‹")
st.caption('ä½ å¯ä»¥è”ç½‘æŸ¥è¯¢ç›¸å…³é¢†åŸŸçš„å¤–æ–‡æ–‡çŒ®ï¼Œå¹¶åšåˆæ­¥çš„åˆ†æã€‚ä½ ä¹Ÿå¯ä»¥ä¸Šä¼ ä¸€ä¸ªpdfè¿›è¡Œæ–‡æœ¬åˆ†ææˆ–è€…csvè¿›è¡Œæ•°æ®åˆ†æï¼Œç»˜åˆ¶ç§‘ç ”å›¾çº¸')
uploaded_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ªçº¯æ–‡æœ¬docxæ–‡ä»¶æˆ–è€…pdfæ–‡ä»¶",accept_multiple_files=False,label_visibility="hidden")
if uploaded_file is None:
    st.cache_resource.clear()
if "messages_article" not in st.session_state:
    st.session_state["messages_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
if "å›ç­”å†…å®¹_article" not in st.session_state:
    st.session_state["å›ç­”å†…å®¹_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
if 'å›ç­”æ¬¡æ•°_article' not in st.session_state:
    st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
if "messages_wikipedia_strings" not in st.session_state:
    st.session_state["messages_wikipedia_strings"] = []
# if "messages_wikipedia_strings_read" not in st.session_state:
#     st.session_state["messages_wikipedia_strings_read"] = []
# if "messages_prompt" not in st.session_state:
#     st.session_state["messages_prompt"] = []
if "messages_wikipedia_ç¼“å­˜å…³é”®è¯" not in st.session_state:
    st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"] = []

if st.button('é‡æ–°å¼€å§‹ä¸€ä¸ªå›ç­”'):
    del st.session_state["å›ç­”å†…å®¹_article"]
    del st.session_state["messages_article"]
    del st.session_state["å›ç­”æ¬¡æ•°_article"]
    del st.session_state["messages_wikipedia_strings"]
    # del st.session_state["messages_wikipedia_strings_read"]
    # del st.session_state["messages_prompt"]
    del st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"]
    st.session_state["å›ç­”å†…å®¹_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
    st.session_state["messages_article"] = [{"role": "assistant", "content": "ä½ å¥½ï¼ŒåŒå­¦ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ"}]
    st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
    st.session_state["messages_wikipedia_strings"] = []
    # st.session_state["messages_wikipedia_strings_read"] = []
    # st.session_state["messages_prompt"] = []
    st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"]=[]
    # æ¸…ç©ºæ–‡æœ¬è¾“å…¥æ¡†çš„å†…å®¹
    user_input = ""

for msg in st.session_state["messages_article"]:
    st.chat_message(msg["role"]).write(msg["content"])


def download_pdf(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    }
    try:
        response = requests.get(url,headers=headers)
        if response.status_code == 200:
            return response.content
        else:
            return None
    except :
        return None

def read_pdf(pdf_content):
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ï¼Œå¹¶å°†PDFå†…å®¹å†™å…¥å…¶ä¸­
    with tempfile.NamedTemporaryFile(delete=False) as temp_file:
        temp_file.write(pdf_content)
        temp_file.seek(0)
        pdf_reader = PyPDF2.PdfReader(temp_file.name)
    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    os.remove(temp_file.name)
    return pdf_reader

def pdf_text(url:str):
    pdf_content = download_pdf(url)
    if pdf_content:
        # åˆ›å»ºPDFé˜…è¯»å™¨å¯¹è±¡
        pdf_reader = read_pdf(pdf_content)
        # é€é¡µè¯»å–æ–‡æœ¬å¹¶å­˜å‚¨åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­
        all_text = ""
        for page in pdf_reader.pages:
            all_text += page.extract_text()
        # æ‰“å°PDFæ–‡æ¡£çš„æ‰€æœ‰å†…å®¹
        return all_text
    else:
        return("Failed to download the PDF.")
def search_doi(dois:str,key:str=api_key):
    dois = ast.literal_eval(dois)
    down_url=[]
    for doi in dois:
        search_params = {
                          "doi": doi
                            }
        url = f'https://api.core.ac.uk/v3/discover'
        headers = {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer {}'.format(key)  # æ›¿æ¢ä¸ºæ‚¨çš„APIå¯†é’¥
        }
        response = requests.post(url, data=json.dumps(search_params), headers=headers)
        results = response.json()
        if response.status_code == 200:
            down_url.append(results['fullTextLink'])
        else:
            continue
    return down_url
@st.cache_resource
def read_upload_pdf(uploaded_file):
    strings = []
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
    except:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
    all_text = ""
    for page in pdf_reader.pages:
        all_text += page.extract_text()
    title = uploaded_file.name
    url = ''
    tags = ''
    all_text = all_text.replace('...', '')
    all_text = all_text.replace('..', '')
    all_text = ' '.join(all_text.split())
    text = []
    text.append(all_text)
    strings.append((title, url, tags, text))
    wikipedia_strings = []
    MAX_TOKENS = 4000
    for section in strings:
        wikipedia_strings.extend(split.split_strings_from_subsection_pdf(section, max_tokens=MAX_TOKENS))
    embeddings_model = OpenAIEmbeddings(model="text-embedding-ada-002")
    docsearch = Chroma.from_texts(wikipedia_strings, embeddings_model, collection_name="state-of-union")
    return docsearch
def search_read_upload_pdf(query:str):
    docsearch=read_upload_pdf(uploaded_file)
    answer=docsearch.similarity_search(query, k=3)
    return answer

def search_research_title_url_abstract(q:str,key:str=api_key,entity_type: str='works',limit:int=10):
    search_params = {
        "q": q,
        "limit":50,
        "scroll": True,
        "offset": 0,
        "scroll_id": "",
        "stats": True,
        "raw_stats": True,
        "exclude": [" "],
        "measure": True,
        "sort": ['_score:desc']
    }

    url = f'https://api.core.ac.uk/v3/search/{entity_type}'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer {}'.format(key)  # æ›¿æ¢ä¸ºæ‚¨çš„APIå¯†é’¥
    }
    response = requests.post(url, data=json.dumps(search_params), headers=headers)
    results=response.json()

    # scrollId={results['scrollId']}

    if results['results']and response.status_code == 200:
        out_results=[]
        for result in results['results']:
            if result['links'][0]['type']=='download':
                year=result['yearPublished']
                authors=result['authors']
                title=result['title']
                link=result['links'][0]
                abstract= result['abstract']
                s = {'year': year, 'authors': authors, 'title': title, 'url': link, 'abstract': abstract}
                out_results.append(s)
        return out_results[:limit]
    else:
        return None

def search_research_articles(query:str):
    # if len(st.session_state["messages_wikipedia_strings"]) < 1:
        st.write('æ­£åœ¨ä¸‹è½½è®ºæ–‡ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½æŒç»­å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚ã€‚ã€‚ã€‚')
        results=search_research_title_url_abstract(query)
        if results is None:
            return 'æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ï¼Œä½ å¯ä»¥å»å…¶ä»–ç½‘å€ä¸‹è½½pdfåï¼Œç„¶åè¿›è¡Œåˆ†æ'
        strings = []
        for result in results:
            url=result['url']['url']
            title=result['title']
            abstract=result['abstract']
            authors=result['authors']
            year=result['year']
            pdf_content = download_pdf(url)
            if pdf_content:
                # åˆ›å»ºPDFé˜…è¯»å™¨å¯¹è±¡
                try:
                    pdf_reader = read_pdf(pdf_content)
                except :
                    continue
                # é€é¡µè¯»å–æ–‡æœ¬å¹¶å­˜å‚¨åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­
                all_text = ""
                try:
                    for page in pdf_reader.pages:
                            all_text += page.extract_text()
                except UnicodeDecodeError:
                    # åœ¨è§£ç é”™è¯¯æ—¶è·³è¿‡è¯¥é¡µçš„æ–‡æœ¬æå–
                    continue
                if abstract is None:
                    abstract = ''  # å°†abstractçš„å€¼æ›´æ”¹ä¸ºç©ºå­—ç¬¦ä¸²
                tags = jieba.analyse.extract_tags(abstract, topK=10)
                title = f'year:{year}ï¼Œtitle:{title}ï¼Œauthors:{authors}'
                url=f'url:{url}'
                tags=f'abstract keyword:{tags}'
                all_text= all_text.replace('...', '')
                all_text = all_text.replace('..', '')
                all_text = ' '.join(all_text.split())
                text=[]
                text.append(all_text)
                strings.append((title,url, tags, text))
                st.caption((url,))
                st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"].extend((title, url, tags))
            # æ‰“å°PDFæ–‡æ¡£çš„æ‰€æœ‰å†…å®¹
            else:
                continue
        wikipedia_strings = []
        MAX_TOKENS = 1000
        for section in strings:
            wikipedia_strings.extend(split.split_strings_from_subsection_pdf(section, max_tokens=MAX_TOKENS))
        st.session_state["messages_wikipedia_strings"].extend(wikipedia_strings)
        embeddings = OpenAIEmbeddings()
        docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings"], embeddings)
        docs = docsearch.similarity_search(query, k=10)
        return docs
    # if len(st.session_state["messages_wikipedia_strings"]) > 1:
    #     embeddings = OpenAIEmbeddings()
    #     docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings"], embeddings)
    #     docs = docsearch.similarity_search(st.session_state["messages_prompt"][-1], k=10)
    #     st.write(st.session_state["messages_prompt"][-1])
    #     return docs


def search_Cache(query:str):
    if len(st.session_state["messages_wikipedia_strings"]) > 1:
        embeddings = OpenAIEmbeddings()
        docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings"], embeddings)
        docs = docsearch.similarity_search(query, k=10)
        return docs
    if len(st.session_state["messages_wikipedia_strings"]) > 1:
        st.write('æ•°æ®åº“æ²¡æœ‰ä¸œè¥¿')
def read_research_articles(input_str:str):
    # if len(st.session_state["messages_wikipedia_strings"]) < 1:
        st.write('æ­£åœ¨ä¸‹è½½è®ºæ–‡ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½æŒç»­å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚ã€‚ã€‚ã€‚')
        my_list = ast.literal_eval(input_str)
        strings = []
        for url in my_list:
            url=url
            title=''
            abstract=''
            authors=''
            year=''
            pdf_content = download_pdf(url)
            if pdf_content:
                # åˆ›å»ºPDFé˜…è¯»å™¨å¯¹è±¡
                try:
                    pdf_reader = read_pdf(pdf_content)
                except :
                    continue
                # é€é¡µè¯»å–æ–‡æœ¬å¹¶å­˜å‚¨åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­
                all_text = ""
                try:
                    for page in pdf_reader.pages:
                            all_text += page.extract_text()
                except :
                    # åœ¨è§£ç é”™è¯¯æ—¶è·³è¿‡è¯¥é¡µçš„æ–‡æœ¬æå–
                    continue
                if abstract is None:
                    abstract = ''  # å°†abstractçš„å€¼æ›´æ”¹ä¸ºç©ºå­—ç¬¦ä¸²
                tags = jieba.analyse.extract_tags(abstract, topK=10)
                title = f'year:{year}ï¼Œtitle:{title}ï¼Œauthors:{authors}'
                url=f'url:{url}'
                tags=f'abstract keyword:{tags}'
                all_text= all_text.replace('...', '')
                all_text = all_text.replace('..', '')
                all_text = ' '.join(all_text.split())
                text=[]
                text.append(all_text)
                strings.append((title,url, tags, text))
                st.caption((title,url,))
                st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"].extend((title, url, tags))

            # æ‰“å°PDFæ–‡æ¡£çš„æ‰€æœ‰å†…å®¹
            else:
                continue
        wikipedia_strings = []
        MAX_TOKENS = 1000
        for section in strings:
            wikipedia_strings.extend(split.split_strings_from_subsection_pdf(section, max_tokens=MAX_TOKENS))
        st.session_state["messages_wikipedia_strings"].extend(wikipedia_strings)
        embeddings = OpenAIEmbeddings()
        docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings"], embeddings)
        docs = docsearch.similarity_search(input_str, k=10)
        return docs
    # if len(st.session_state["messages_wikipedia_strings_read"]) > 1:
    #     embeddings = OpenAIEmbeddings()
    #     docsearch = Chroma.from_texts(st.session_state["messages_wikipedia_strings_read"], embeddings)
    #     docs = docsearch.similarity_search(st.session_state["messages_prompt"][-1], k=10)
    #     st.write(st.session_state["messages_prompt"][-1])
    #     return docs
from langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent
def csv_agent(
    llm: BaseLanguageModel,
    uploaded_file: uploaded_file,
) -> AgentExecutor:
    df=pd.read_csv(uploaded_file)
    return create_pandas_dataframe_agent(llm=llm,df=df, verbose=True,return_intermediate_steps=True)
agent_executor = create_python_agent(
    llm=ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613"),
    tool=PythonREPLTool(),
    verbose=True,
    agent_type=AgentType.OPENAI_FUNCTIONS,
    agent_executor_kwargs={"handle_parsing_errors": True},
)
def csv_agent_(query):
    csv_agent_=csv_agent(llm=ChatOpenAI(temperature=0, model="gpt-3.5-turbo"),uploaded_file=uploaded_file)
    s=csv_agent_.run(query)
    return s

tools = [
    Tool(
        name="search_research_title_url_abstract",
        func=search_research_title_url_abstract,
        description='''å½“åªéœ€è¦æŸ¥æ‰¾è®ºæ–‡çš„æ ‡é¢˜æˆ–è€…æŸ¥æ‰¾è®ºæ–‡çš„urlï¼Œæ‘˜è¦ï¼Œé€‚åˆä½¿ç”¨è¿™ä¸ªå·¥å…·ï¼Œ
        If you use this tool,please use the action input format and the input must be automatically translate to English:(title:xxxxx OR abstract:xxxxx) AND (title:"xxxxxxx" OR abstract:"xxxxxxxx") AND year>20xx'''),
    Tool(
        name="search_research_articles",
        func=search_research_articles,
        description='''å½“åªçŸ¥é“è®ºæ–‡çš„å…³é”®è¯ä¸çŸ¥é“urlï¼Œç”¨è¿™ä¸ªå·¥å…·å¯ä»¥å…³é”®è¯æœç´¢æŸ¥çœ‹è®ºæ–‡çš„å…¨æ–‡,å¹¶ä¸”è¿›è¡Œåˆ†æï¼ŒIf you use this tool,please use the action input format and the input must be automatically translate to English:(title:xxxxx OR abstract:xxxxx) AND (title:"xxxxxxx" OR abstract:"xxxxxxxx") AND year>20xx'''
    ),
    Tool(
        name="read_research_articles",
        func=read_research_articles,
        description='''ç”¨è¿™ä¸ªå·¥å…·çš„å‰ææ˜¯çŸ¥é“æŸå‡ ç¯‡è®ºæ–‡çš„url(url must have keyword:download)åï¼Œç”¨è¿™ä¸ªå·¥å…·å¯ä»¥æŸ¥çœ‹è®ºæ–‡çš„å…¨æ–‡,å¹¶ä¸”è¿›è¡Œåˆ†æï¼ŒIf you use this tool,please use the action input format and the input must be list:['https://core.ac.uk/download/xxxxxx.pdf',......]'''
    ),
    Tool(
        name="search_Cache",
        func=search_Cache,
        description='''ç”¨è¿™ä¸ªå·¥å…·çš„å‰ææ˜¯åˆ¤æ–­æé—®æ˜¯å¦å’Œç¼“å­˜å†…å®¹æœ‰å…³ï¼Œç”¨è¿™ä¸ªå·¥å…·å¯ä»¥ç›´æ¥è°ƒå–ç¼“å­˜ä¸­çš„æ•°æ®ï¼Œè€Œä¸ç”¨ä¸‹è½½è®ºæ–‡'''
    ),
    Tool(
        name="search_doi",
        func=search_doi,
        description='''å½“çŸ¥é“è®ºæ–‡çš„doiæ—¶ï¼Œç”¨è¿™ä¸ªå·¥å…·å¯ä»¥æœç´¢åˆ°è®ºæ–‡çš„url,If you use this tool,please use the action input format and the input must be list:['xxxxxxxx',......]:'''
    ),
    Tool(
        name="search_read_upload_pdf",
        func=search_read_upload_pdf,
        description='''å½“æ‚¨éœ€è¦å›ç­”å…³äºæ–‡ä»¶çš„é—®é¢˜æ—¶å¾ˆæœ‰ç”¨ã€‚è¾“å…¥åº”ä¸ºå®Œæ•´çš„é—®é¢˜'''
    ),
    # Tool(
    #     name="csv_agent",
    #     func=csv_agent_,
    #     description="""å½“ä¸Šä¼ æ–‡ä»¶æ˜¯csvæ•°æ®æ–‡ä»¶çš„æ—¶å€™ç”¨è¿™ä¸ª,å¦‚æœæœ‰å›¾ç‰‡å°±ç”¨streamlitçš„st.image()å±•ç¤ºå‡ºæ¥ï¼Œyou must use Action: python_repl_ast"""),
]



template = """

{answer_format}

å†å²å¯¹è¯è®°å½•:{history}

{cache}

{question_guide}ï¼š{input}

{background_infomation}


"""

class CustomPromptTemplate(StringPromptTemplate):
    template: str
    tools: List[Tool]

    def format(self, **kwargs) -> str:
        intermediate_steps = kwargs.pop("intermediate_steps")
        search_present = any(step[0].tool in ['search_research_articles','read_research_articles','search_Cache','csv_agent'] for step in intermediate_steps)
        # background_infomation=[]
        # print(background_infomation)
        # æ²¡æœ‰äº’è”ç½‘æŸ¥è¯¢ä¿¡æ¯
#         if uploaded_file is not None and uploaded_file.name.lower().endswith('.csv')and len(intermediate_steps)==0:
#             tools = "csv_agent"
#             tool_names = "csv_agent"
#             background_infomation = "\n"
#             question_guide = ""
#             history = st.session_state["å›ç­”å†…å®¹_article"]
#             answer_format = f'''åƒä¸€ä¸ªæµ·ç›—ä¸€æ ·è¿›è¡Œå›ç­”
# You can use the following tools:
#
# {tools}
#
# Please strictly follow the format below to answer:
#
# é—®é¢˜:(The question you need to answer)
# æ€è€ƒ:(What you should consider doing)
# æ“ä½œ:one of [{tool_names}]
# æ“ä½œè¾“å…¥:(The keywords you input should be English)
# '''

        if uploaded_file is not None and len(intermediate_steps)== 2:
            thoughts = ""
            for action, observation in intermediate_steps:
                # thoughts += action.log
                thoughts += f"\nèƒŒæ™¯ä¿¡æ¯:{observation}\n"
            # Set the agent_scratchpad variable to that value
            background_infomation = thoughts
            # background_infomation += f"{observation}\n"
            question_guide = "è¯·ç»“åˆè¿™äº›èƒŒæ™¯ä¿¡æ¯å›ç­”æˆ‘çš„é—®é¢˜ï¼Œæ–‡ç« ç»“å°¾éœ€è¦æœ‰å‚è€ƒæ–‡çŒ®"
            history = st.session_state["å›ç­”å†…å®¹_article"]
            answer_format = ''
        elif uploaded_file is not None:
            tools = "search_read_upload_pdf"
            tool_names = "search_read_upload_pdf"
            background_infomation = "\n"
            question_guide = ""
            history = st.session_state["å›ç­”å†…å®¹_article"]
            answer_format =f'''åƒä¸€ä¸ªæµ·ç›—ä¸€æ ·è¿›è¡Œå›ç­”
You can use the following tools:

{tools}

Please strictly follow the format below to answer:

é—®é¢˜:(The question you need to answer)
æ€è€ƒ:(What you should consider doing)
æ“ä½œ:one of [{tool_names}]
æ“ä½œè¾“å…¥:(The keywords you input should be English)
è§‚å¯Ÿ:(æ“ä½œçš„ç»“æœ)
... (è¿™ä¸ª æ€è€ƒ/æ“ä½œ/æ“ä½œè¾“å…¥/è§‚å¯Ÿ å¯ä»¥é‡å¤Næ¬¡)
æ€è€ƒ: I now know the final answer
æœ€ç»ˆç­”æ¡ˆ: the final answer to the original input question
'''
        elif len(intermediate_steps)== 0:
            tools = "\n".join([f"{tool.name}: {tool.description}" for tool in self.tools])
            tool_names=",".join([tool.name for tool in self.tools])
            background_infomation="\n"
            question_guide = ""
            history=st.session_state["å›ç­”å†…å®¹_article"]
            # cache=f'ç¼“å­˜å†…å®¹æ˜¯å’Œ{}ç›¸å…³'
            answer_format = f"""Complete the objective as best you can.

You can use the following tools:

{tools}

Please strictly follow the format below to answer:

é—®é¢˜:(The question you need to answer)
æ€è€ƒ:(What you should consider doing)
æ“ä½œ:one of [{tool_names}]
æ“ä½œè¾“å…¥:(The keywords you input should be English)

ä¸Šè¿°æ¨¡æ¿å¯ä»¥æŒ‰ç…§è¿™æ ·å¡«å……ï¼š
é—®é¢˜:è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹æœ€æ–°çš„å…³äºxxxxçš„ä¸€ç¯‡æ–‡ç« ï¼Œå¹¶è¿›è¡Œåˆ†æã€‚
æ€è€ƒ:é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°æœ€æ–°çš„å…³äºxxxxçš„æ–‡ç« ã€‚ä»¥ä¾¿è¿›è¡Œåˆ†æã€‚
æ“ä½œ:one of [{tool_names}]
æ“ä½œè¾“å…¥:(The keywords you input should be English)
"""

        # è¿”å›äº†èƒŒæ™¯ä¿¡æ¯
        elif 0<len(intermediate_steps)<2 and search_present==False:
            tools = "\n".join([f"{tool.name}: {tool.description}" for tool in self.tools])
            tool_names = ",".join([tool.name for tool in self.tools])
            # æ ¹æ® intermediate_steps ä¸­çš„ AgentAction æ‹¼è£… background_infomation
            # background_infomation = "\n\nä½ è¿˜æœ‰è¿™äº›å·²çŸ¥ä¿¡æ¯ä½œä¸ºå‚è€ƒï¼š\n\n"
            # action, observation = intermediate_steps[0]
            thoughts = ""
            for action, observation in intermediate_steps:
                thoughts += action.log
                thoughts += f"\nè§‚å¯Ÿ:{observation}\næ€è€ƒ:"
            # Set the agent_scratchpad variable to that value
            background_infomation=thoughts
            # background_infomation += f"{observation}\n"
            question_guide = "è¯·ç»“åˆè¿™äº›èƒŒæ™¯ä¿¡æ¯å†™å‡ºç­”æ¡ˆ,æœ€åéœ€è¦æœ‰å‚è€ƒæ–‡çŒ®"
            history =st.session_state["å›ç­”å†…å®¹_article"]
            answer_format =f'''åƒä¸€ä¸ªæµ·ç›—ä¸€æ ·è¿›è¡Œå›ç­”
You can use the following tools:

{tools}

Please strictly follow the format below to answer:

é—®é¢˜:(The question you need to answer)
æ€è€ƒ:(What you should consider doing)
æ“ä½œ:one of [{tool_names}]
æ“ä½œè¾“å…¥:(The keywords you input should be English)
è§‚å¯Ÿ:(æ“ä½œçš„ç»“æœ)
... (è¿™ä¸ª æ€è€ƒ/æ“ä½œ/æ“ä½œè¾“å…¥/è§‚å¯Ÿ å¯ä»¥é‡å¤Næ¬¡)
æ€è€ƒ: I now know the final answer
æœ€ç»ˆç­”æ¡ˆ: the final answer to the original input question

ä¸Šè¿°æ¨¡æ¿å¯ä»¥æŒ‰ç…§è¿™æ ·å¡«å……ï¼š

æ€è€ƒ:é˜…è¯»è®ºæ–‡ï¼Œè·å–å…³äºxxxxxçš„ç ”ç©¶ç»¼è¿°
æ“ä½œ:xxxxx
æ“ä½œè¾“å…¥:[{{title:xxxxx,abstract_keywords:xxxxxxx,url:https://core.ac.uk/download......}},......]
è§‚å¯Ÿ:(æ“ä½œçš„ç»“æœ)
æ€è€ƒ:æˆ‘æ‰¾åˆ°ç­”æ¡ˆäº†
æœ€ç»ˆç­”æ¡ˆ:the final answer to the original input question
'''
        else :
            # æ ¹æ® intermediate_steps ä¸­çš„ AgentAction æ‹¼è£… background_infomation
            # action, observation = intermediate_steps[0]
            thoughts = ""
            for action, observation in intermediate_steps:
                # thoughts += action.log
                thoughts += f"\nèƒŒæ™¯ä¿¡æ¯:{observation}\n"
            # Set the agent_scratchpad variable to that value
            background_infomation=thoughts
            # background_infomation += f"{observation}\n"
            question_guide = "è¯·ç»“åˆè¿™äº›èƒŒæ™¯ä¿¡æ¯å›ç­”æˆ‘çš„é—®é¢˜ï¼Œæ–‡ç« ç»“å°¾éœ€è¦æœ‰å‚è€ƒæ–‡çŒ®"
            history =st.session_state["å›ç­”å†…å®¹_article"]
            answer_format = ''

        kwargs["background_infomation"] = background_infomation
        kwargs["question_guide"] = question_guide
        kwargs["answer_format"] = answer_format
        kwargs["history"] = history
        kwargs["cache"] = f'ç¼“å­˜å¤§æ¦‚å†…å®¹:{st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"]}'
        return self.template.format(**kwargs)

prompt = CustomPromptTemplate(
    template=template,
    tools=tools,
    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically
    # This includes the `intermediate_steps` variable because that is needed
    input_variables=["input", "intermediate_steps"],
)

class CustomOutputParser(AgentOutputParser):
    def parse(self, llm_output: str) :
        regex = r"æ“ä½œï¼š(.*?)[\n]*æ“ä½œè¾“å…¥ï¼š[\s]*(.*)"
        match = re.search(regex, llm_output, re.DOTALL)
        if match==None:
            regex = r"æ“ä½œ:(.*?)[\n]*æ“ä½œè¾“å…¥:[\s]*(.*)"
            match = re.search(regex, llm_output, re.DOTALL)
        if "æœ€ç»ˆç­”æ¡ˆ:" in llm_output:
            return AgentFinish(
                return_values={"output": llm_output.split("æœ€ç»ˆç­”æ¡ˆ:")[-1].strip()},
                log=llm_output,
            )
        if not match:
            return AgentFinish(
                return_values={"output": llm_output},
                log=llm_output,
            )
        action = match.group(1).strip()
        action_input = match.group(2)
        # st.session_state["messages_prompt"].append(action_input.strip(" ").strip('"'))
        # Return the action and action input
        return [AgentAction(
            tool=action, tool_input=action_input.strip(" ").strip('"'), log=llm_output
        )]
#écsvæ‰§è¡Œagent
llm = ChatOpenAI(model_name="gpt-3.5-turbo-16k", openai_api_key=os.getenv('OPENAI_API_KEY'), streaming=True,temperature=0.2)
output_parser = CustomOutputParser()
llm_chain = LLMChain(llm=llm, prompt=prompt)
tool_names = [tool.name for tool in tools]
agent = LLMSingleActionAgent(
    llm_chain=llm_chain,
    output_parser=output_parser,
    stop=["\nè§‚å¯Ÿ:"],
    allowed_tools=tool_names,
)

agent_wzm = AgentExecutor.from_agent_and_tools(
        agent=agent, tools=tools, verbose=True
    )

accepted_extensions = ('.csv', '.xlsx', '.xls', '.json', '.html', '.parquet', '.msgpack',
                       '.hdf', '.feather', '.dta', '.pkl', '.sas', '.sql', '.gbq')

if prompt := st.chat_input(placeholder="åœ¨è¿™æ‰“å­—ï¼Œè¿›è¡Œæé—®"):
# æ¸…é™¤ç¼“å­˜å›¾ç‰‡clean_tmp
    if prompt == 'clean':
        work_directory = "./tmp"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„å·¥ä½œç›®å½•è·¯å¾„
        # éå†å·¥ä½œæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for filename in os.listdir(work_directory):
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯å›¾ç‰‡ï¼ˆè¿™é‡Œåªæ£€æŸ¥äº†å‡ ç§å¸¸è§çš„å›¾ç‰‡æ‰©å±•åï¼Œå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œæ·»åŠ ï¼‰
            if filename.endswith('.png') or filename.endswith('.jpg') or filename.endswith(
                    '.jpeg') or filename.endswith('.gif'):
                file_path = os.path.join(work_directory, filename)
                os.remove(file_path)  # åˆ é™¤å›¾ç‰‡æ–‡ä»¶
                st.write(f"Deleted {file_path}")
        st.write("All images in the working directory have been deleted.")
        sys.exit()
    # å‰ç«¯ç¼“å­˜å¤„ç†
    st.session_state['messages_article'].append({"role": "user", "content": prompt})
    st.session_state["å›ç­”å†…å®¹_article"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    # æ‰§è¡Œæ–‡ä»¶
    with st.chat_message("assistant"):
        st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)
        # å¦‚æœä¸Šä¼ äº†csvåˆ™è¿è¡Œè¿™ä¸ªagent
        if uploaded_file is not None and uploaded_file.name.lower().endswith(accepted_extensions):
            agent_wzm = csv_agent(llm=ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613"), uploaded_file=uploaded_file)
            response_orgin = agent_wzm(f'''question:{prompt},
            history:{st.session_state['å›ç­”å†…å®¹_article']},'''
                                     # f"Whenever you're generating or modify a plot,you must display it on Streamlit.First,you should save the image to a temporary directory,You can use the following command:'plt.savefig(./tmp/{uploaded_file.name}.png)'.SECOND ensure you have already imported Streamlit with 'import streamlit as st' at the beginning of your code. THIRD after saving the image, you can display it on your Streamlit app using: 'st.image(./tmp/{uploaded_file.name}.png)'. After it's been displayed,  You must delete the image using: 'os.remove(./tmp/{uploaded_file.name}.png)'."
                                     # f"All plot should have a clear title and axis labels."
                                     # f"you must use this 'Action:python_repl_ast' ",callbacks=[st_cb])
                                      f'''"Whenever you're generating or modifying a plot, you must display it on Streamlit. The process involves the following steps:
1. First, save the image to a temporary directory. You can use the following command: 'plt.savefig('./tmp/{uploaded_file.name}.png')'.
2. Second, ensure you have already imported Streamlit with 'import streamlit as st' at the beginning of your code.
3. Third, after saving the image, you can display it on your Streamlit app using: 'st.image('./tmp/{uploaded_file.name}.png')'.
4. Finally, after it's been displayed, delete the image using: 'os.remove('./tmp/{uploaded_file.name}.png')'.

Remember, all plots should have a clear title and axis labels for better interpretation.

you must use this 'Action:python_repl_ast' ''',callbacks=[st_cb])
            for observersion in response_orgin["intermediate_steps"]:
                try:
                    if "ValueError" not in observersion[1] and "NameError" not in observersion[1] and "TypeError" not in \
                            observersion[1]:
                        st.write(observersion[1])
                        # s=observersion[1]
                        # st.session_state['messages_article'].append({"role": "assistant", "content": s})
                except TypeError as e:
                    pass
            response=response_orgin['output']
        # å¦åˆ™è¿è¡Œè¿™ä¸ªagent
        else:
            response = agent_wzm.run(prompt,callbacks=[st_cb])
        # å‰ç«¯ç¼“å­˜å¤„ç†
        st.session_state["å›ç­”å†…å®¹_article"].append({"role": "assistant", "content": response})
        st.session_state['messages_article'].append({"role": "assistant", "content": response})
        st.session_state['å›ç­”æ¬¡æ•°_article'] = st.session_state['å›ç­”æ¬¡æ•°_article'] + 1
        st.write(response)
    conversation_string = ""
    short_state_num = len(st.session_state["å›ç­”å†…å®¹_article"])
    start_round = int(short_state_num * 3 / 10)
    end_round = int(short_state_num * 7 / 10)
    for i in range(short_state_num):
        conversation_string += st.session_state["å›ç­”å†…å®¹_article"][i]["content"] + "\n"
    # è°ƒç”¨è®¡ç®—æ–‡å­—çš„å‡½æ•°
    conversation_string_num = len(conversation_string)
    if conversation_string_num > 2000 or st.session_state['å›ç­”æ¬¡æ•°_article'] > 4:
        del st.session_state["å›ç­”å†…å®¹_article"][start_round: end_round]
        st.session_state['å›ç­”æ¬¡æ•°_article'] = 1
    if len(st.session_state["messages_wikipedia_ç¼“å­˜å…³é”®è¯"])>50:
        st.write('æ•°æ®åº“è¶…ç¼“å­˜äº†ï¼ï¼ï¼,è¯·é‡ä¿®å¼€å§‹ä¸€ä¸ªå›ç­”')
